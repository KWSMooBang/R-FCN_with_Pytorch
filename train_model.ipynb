{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pprint # pretty print module: 데이터를 보기 좋게 출력할 때 사용\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets.CustomVOCDetection import CustomVOCDetection\n",
    "from roi_data_layer.roi_data_layer import combined_roidb\n",
    "from roi_data_layer.roibatchLoader import roibatchLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define arguments\n",
    "n_epochs = 20\n",
    "save_dir = 'save'\n",
    "is_ohem = True\n",
    "batch_size = 1\n",
    "lr = 0.001\n",
    "lr_decay_step = 5\n",
    "lr_decay_gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sampler(Sampler):\n",
    "    def __init__(self, train_size, batch_size):\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batch = int(train_size / batch_size)\n",
    "        self.range = torch.arange(0, batch_size).view(1, batch_size).long()\n",
    "        self.leftover_flag = False\n",
    "        if train_size % batch_size:\n",
    "            self.leftover = torch.arange(self.num_per_batch*batch_size, train_size).long()\n",
    "            self.leftover_flag = True\n",
    "\n",
    "    def __iter__(self):\n",
    "        rand_batch_idx = torch.randperm(self.num_batch).view(-1, 1) * self.batch_size\n",
    "        self.rand_batch_idx = rand_batch_idx.expand(self.num_batch, self.batch_size) + self.range\n",
    "\n",
    "        self.rand_idx = self.rand_num.view(-1)\n",
    "\n",
    "        if self.leftover_flag:\n",
    "            self.rand_idx = torch.cat((self.rand_idx, self.leftover), 0)\n",
    "\n",
    "        return iter(self.rand_idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu is being used.\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'gpu'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "print(f\"{device} is being used.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pascal_voc_2012_train gt roidb loaded from /workspaces/R-FCN_with_Pytorch/datasets/cache/pascal_voc_2012_train_gt_roidb.pkl\n",
      "Preparing training data...\n",
      "Done\n",
      "before filtering, there are 5717 images...\n",
      "after filtering, there are 5717 images...\n",
      "5717 roidb entries\n"
     ]
    }
   ],
   "source": [
    "pascal_voc_dataset = CustomVOCDetection(\n",
    "    root=os.path.abspath(os.path.join(os.getcwd(), 'datasets')),\n",
    "    year='2012',\n",
    "    image_set='train',\n",
    "    download=False,\n",
    ")\n",
    "roidb, ratio_list, ratio_index = combined_roidb(pascal_voc_dataset)\n",
    "train_size = len(roidb)\n",
    "\n",
    "print(\"{:d} roidb entries\".format(train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'areas': array([139946.,  16368.], dtype=float32),\n",
      " 'boxes': array([[ 52,  86, 470, 419],\n",
      "       [157,  43, 288, 166]], dtype=uint16),\n",
      " 'flipped': False,\n",
      " 'gt_classes': array([13, 15], dtype=int32),\n",
      " 'gt_difficult': array([0, 0], dtype=int32),\n",
      " 'gt_ious': <2x21 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 2 stored elements in Compressed Sparse Row format>,\n",
      " 'height': 442,\n",
      " 'image': '/workspaces/R-FCN_with_Pytorch/datasets/VOCdevkit/VOC2012/JPEGImages/2008_000008.jpg',\n",
      " 'image_id': 0,\n",
      " 'max_classes': array([13, 15]),\n",
      " 'max_ious': array([1., 1.], dtype=float32),\n",
      " 'need_crop': 0,\n",
      " 'width': 500}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(roidb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(os.getcwd(), 'save')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_batch = sampler(train_size, batch_size)\n",
    "\n",
    "dataset = roibatchLoader(roidb, ratio_list, ratio_index, batch_size, \n",
    "                         pascal_voc_dataset.num_classes, training=True)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tensor holder \n",
    "image_data = torch.FloatTensor(1)\n",
    "image_info = torch.FloatTensor(1)\n",
    "num_boxes = torch.LongTensor(1)\n",
    "gt_boxes = torch.FloatTensor(1)\n",
    "\n",
    "# ship to cuda\n",
    "if device == 'gpu':\n",
    "    image_data = image_data.cuda()\n",
    "    image_info = image_info.cuda()\n",
    "    num_boxes = num_boxes.cuda()\n",
    "    gt_boxes = gt_boxes.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /home/codespace/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "base_model = torchvision.models.resnet101(weights=\"IMAGENET1K_V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
